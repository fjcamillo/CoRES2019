{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/google-automl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML by Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am\n",
    "\n",
    "### FJ Camillo\n",
    "\n",
    "What I do:\n",
    "- Build microservices using python and javascript\n",
    "- Build IOT projects\n",
    "- Do 3D Models and Animations\n",
    "- Environment Sketches\n",
    "\n",
    "![title](resources/45823323_2423803294303443_4447947524208066560_n.jpg)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\n",
    "### ML ( Machine Learning )\n",
    "\n",
    "\n",
    "### AutoML\n",
    "Automated methods for model selection and/or hyperparameter optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to expect\n",
    "\n",
    "This talk will mainly revolve around the basic of each neural network architecture, to have a basic understanding of how AutoML helps solve the problem of current data scientists in building architectures for different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Question](#Questions)\n",
    "1. [Question 1](#Question-1)\n",
    "2. [Question 2](#Question-2)\n",
    "3. [Question 3](#Question-3)\n",
    "\n",
    "[Track 1](#Track-1) <br>\n",
    "If audience is not yet ready to learn about AutoML\n",
    "1. [What is Neural Network](#What-is-neural-network)\n",
    "2. [Parts of a Neural Network](#Parts-of-a-Neural-Network)\n",
    "3. [5 Layer Neural Network](#5-layer-neural-network)\n",
    "4. [Convolutional Neural Network](#Convolutional-neural-network)\n",
    "5. [Different CNN Architectures](#Different-cnn-architectures)\n",
    "6. [Recurrent Neural Network](#recurrent-neural-network)\n",
    "7. [Different RNN Architectures](#different-rnn-architectures)\n",
    "8. [Reinforcement Learning](#reinforcement-learning)\n",
    "\n",
    "[Track 2](#track-2) <br>\n",
    "If audience is ready to learn about AutoML\n",
    "\n",
    "1. [What is Neural Network](#what-is-neural-network)\n",
    "2. [Neural Network Architecture thru the ages](#neural-network-architecture-thru-the-ages)\n",
    "3. [How does one build a Neural Network Architecture](#how-does-one-build-a-neural-network-architecture)\n",
    "4. [Neural Architecture Search](#neural-architecture-search)\n",
    "5. [NASNet](#nasnet)\n",
    "6. [AmoebaNet](#amoebanet)\n",
    "7. [ENAS](#enas)\n",
    "8. [Differential Architecture Search ( DARTS )](#differential-architecture-search)\n",
    "\n",
    "[Main Talk](#main-talk) <br>\n",
    "1. [Neural Architecture Search Simplified](#neural-architecture-search-simplified)\n",
    "2. [Google's Implementation ( Google AutoML )](#Google-AutoML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Can Anyone recognize this?\n",
    "\n",
    "![questionone](resources/images.png)\n",
    "\n",
    "If you recognize this image raise your hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "![questiontwo](resources/Screen-Shot-2018-04-16-at-11.34.51-AM.png)\n",
    "\n",
    "If you recognize this image raise your hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "![questionthree](resources/teaching-recurrent-neural-networks-using-tensorflow-webinar-august-2016-24-638.jpg)\n",
    "\n",
    "If you recognize this image raise your hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Network\n",
    "\n",
    "<i>a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.</i>\n",
    "~Dr. Robert Hecht-Nielsen\n",
    "\n",
    "ANNs are processing devices (algorithms or actual hardware) that are loosely modeled after the neuronal structure of the mamalian cerebral cortex but on much smaller scales. A large ANN might have hundreds or thousands of processor units, whereas a mamalian brain has billions of neurons with a corresponding increase in magnitude of their overall interaction and emergent behavior. Although ANN researchers are generally not concerned with whether their networks accurately resemble biological systems, some have. For example, researchers have accurately simulated the function of the retina and modeled the eye rather well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of a Neural Network\n",
    "\n",
    "Neural networks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output as shown in the graphic \n",
    "\n",
    "See the following image\n",
    "\n",
    "![nn](resources/nn.png)\n",
    "\n",
    "* Input <br>\n",
    "Your clean data comes in here <br><br>\n",
    "\n",
    "* Hidden <br>\n",
    "Computes and assigns weights on features <br><br>\n",
    "\n",
    "* Output <br>\n",
    "Classifies / Regress processed data <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Neural Network\n",
    "\n",
    "The number of the layers are defined by the number of hidden layers, although also take note that some use the total count of layers.\n",
    "\n",
    "So for the image below, we could say that is a 2 Layered Neural Network and also a 4 Layered Neural Network\n",
    "\n",
    "![questionone](resources/images.png)\n",
    "\n",
    "Layering neural networks helped us create various architecture that are fit for specific applications\n",
    "\n",
    "Here are some examples\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Reinforcement Learning (RL)\n",
    "- Transfer Learning (TL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Get Into It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Convolutional Neural Networks are very similar to ordinary Neural Networks they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer.\n",
    "\n",
    "\n",
    "### LeNet5\n",
    "\n",
    "![questiontwo](resources/Screen-Shot-2018-04-16-at-11.34.51-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different CNN Architectures\n",
    "\n",
    "#### AlexNet\n",
    "\n",
    "![alexnet](resources/alexnet.png)\n",
    "\n",
    "\n",
    "#### VGGNet \n",
    "\n",
    "![vggnet](resources/vggnet2017.png)\n",
    "\n",
    "#### GoogleNet ( Inception V1 )\n",
    "\n",
    "![googlenet](resources/googlenet.png)\n",
    "\n",
    "#### ResNet\n",
    "\n",
    "![resnet](resources/resnet.png)\n",
    "\n",
    "#### Inception V3\n",
    "\n",
    "![inceptionv3](resources/inceptionv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "Recurrent neural networks, or RNNs, are a type of artificial neural network that add additional weights to the network to create cycles in the network graph in an effort to maintain an internal state.\n",
    "\n",
    "The promise of adding state to neural networks is that they will be able to explicitly learn and exploit context in sequence prediction problems, such as problems with an order or temporal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different RNN Architectures\n",
    "\n",
    "![questionthree](resources/teaching-recurrent-neural-networks-using-tensorflow-webinar-august-2016-24-638.jpg)\n",
    "\n",
    "#### Unfolded Recurrent Neural Network\n",
    "\n",
    "![rnn-unfold](resources/rnn-unfolded.jpg)\n",
    "\n",
    "### RNN Cells\n",
    "\n",
    "#### LSTM ( Long Short Term Memory )\n",
    "\n",
    "![lstm](resources/lstm-cells.png)\n",
    "\n",
    "#### GruCell \n",
    "\n",
    "![grucell](resources/grucell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "It is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path it should take in a specific situation. Reinforcement learning differs from the supervised learning in a way that in supervised learning the training data has the answer key with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of training dataset, it is bound to learn from its experience.\n",
    "\n",
    "![dqn](resources/dqn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architectures Thru The Ages\n",
    "\n",
    "Neural Network Architectures had been growing deeper and deeper thru the ages.\n",
    "\n",
    "Everything started from 1 perceptron up to the latest deepest architecture we have now ( above 1024 layers )\n",
    "\n",
    "Current improvements in neural network architectures are made possible by the huge development of the hardware we are now using specially graphics cards / TPUs and the huge amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does One Build A Neural Network Architecture\n",
    "\n",
    "Building a Neural Network Architectures is not something easy. We have to consider what we are aiming for and what we have as data\n",
    "\n",
    "As stated from track 1, doing image recognition requires you to convolve neural blocks on the images so that you could learn the patterns created by each pixel, while for sentence generation, RNN works best due to it remembering the past and allowing you to link words or characters thru time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoML provides a way to select models and optimize hyper-parameters. It can also be useful in getting a baseline to know what level of performance is possible for a problem. So does this mean that data scientists can be replaced? Not yet, as we need to keep the context of what else it is that machine learning practitioners do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Architecture Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve covered some of what AutoML is, let’s look at a particularly active subset of the field: neural architecture search. Google CEO Sundar Pichai wrote that, “designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets.”\n",
    "\n",
    "What Pichai refers to as using “neural nets to design neural nets” is known as neural architecture search; typically reinforcement learning or evolutionary algorithms are used to design the new neural net architectures. This is useful because it allows us to discover architectures far more complicated than what humans may think to try, and these architectures can be optimized for particular goals. Neural architecture search is often very computationally expensive.\n",
    "\n",
    "To be precise, neural architecture search usually involves learning something like a layer (often called a “cell”) that can be assembled as a stack of repeated cells to create a neural network:\n",
    "\n",
    "![evolve](resources/evolvable-cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term AutoML jumped to “mainstream” prominence with work by Google AI researchers (paper here) Quoc Le and Barret Zoph, which was featured at Google I/O in May 2017. This work used reinforcement learning to find new architectures for the computer vision problem Cifar10 and the NLP problem Penn Tree Bank, and achieved similar results to existing architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASNet\n",
    "\n",
    "![nasnet](resources/image1.png)\n",
    "\n",
    "This work searches for an architectural building block on a small data set (Cifar10) and then builds an architecture for a large data set (ImageNet). This research was very computationally intensive with it taking 1800 GPU days (the equivalent of almost 5 years for 1 GPU) to learn the architecture (the team at Google used 500 GPUs for 4 days!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AmoebaNet\n",
    "\n",
    "![amoebanet](resources/amoeba.png)\n",
    "\n",
    "This research was even more computationally intensive than NASNet, with it taking the equivalent of 3150 GPU days (the equivalent of almost 9 years for 1 GPU) to learn the architecture (the team at Google used 450 K40 GPUs for 7 days!). AmoebaNet consists of “cells” learned via an evolutionary algorithm, showing that artificially-evolved architectures can match or surpass human-crafted and reinforcement learning-designed image classifiers. After incorporating advances from fast.ai such as an aggressive learning schedule and changing the image size as training progresses, AmoebaNet is now the cheapest way to train ImageNet on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENAS\n",
    "\n",
    "![nasnet](resources/enas.png)\n",
    "\n",
    "used much fewer GPU-hours than previously existing automatic model design approaches, and notably, was 1000x less expensive than standard Neural Architecture Search. This research was done using a single GPU for just 16 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Architecture Search\n",
    "\n",
    "![darts](resources/darts.png)\n",
    "\n",
    "This research was recently released from a team at Carnegie Mellon University and DeepMind, and I’m excited about the idea. DARTS assumes the space of candidate architectures is continuous, not discrete, and this allows it to use gradient-based aproaches, which are vastly more efficient than the inefficient black-box search used by most neural architecture search algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AutoML\n",
    "\n",
    "<i>Today, designing neural nets is extremely time intensive, and requires an expertise that limits its use to a smaller community of scientists and engineers. That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets. We hope AutoML will take an ability that a few PhDs have today and will make it possible in three to five years for hundreds of thousands of developers to design new neural nets for their particular needs.</i>\n",
    "\n",
    "~ Sundar Pichai\n",
    "\n",
    "Although the field of AutoML has been around for years (including open-source AutoML libraries, workshops, research, and competitions), in May 2017 Google co-opted the term AutoML for its neural architecture search. In blog posts accompanying announcements made at the conference Google I/O, Google CEO Sundar Pichai wrote, “That’s why we’ve created an approach called AutoML, showing that it’s possible for neural nets to design neural nets” and Google AI researchers Barret Zoph and Quoc Le wrote “In our approach (which we call “AutoML”), a controller neural net can propose a “child” model architecture…”\n",
    "\n",
    "Google’s Cloud AutoML was announced in January 2018 as a suite of machine learning products. So far it consists of one publicly available product, AutoML Vision, an API that identifies or classifies objects in pictures. According to the product page, Cloud AutoML Vision relies on two core techniques: transfer learning and neural architecture search. Since we’ve already explained neural architecture search, let’s now take a look at transfer learning, and see how it relates to neural architecture search.\n",
    "\n",
    "![automlhead](resources/automl-headlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AutoML Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mv](resources/download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Jump In To It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automl](resources/google-ai.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automl1](resources/automl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoKeras\n",
    "\n",
    "![autokeras](resources/1_n8jC9VAQuED6xhYTcCJ0pQ.png)\n",
    "\n",
    "Auto-Keras is an open source software library for automated machine learning (AutoML). It is developed by DATA Lab at Texas A&M University and community contributors. The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background. Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. https://arxiv.org/pdf/1802.01548.pdf\n",
    "2. https://www.fast.ai/2018/07/12/auto-ml-1/\n",
    "3. https://www.fast.ai/2018/07/16/auto-ml2/\n",
    "4. https://www.fast.ai/2018/04/30/dawnbench-fastai/\n",
    "5. https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "6. https://arxiv.org/pdf/1802.03268.pdf\n",
    "7. http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html\n",
    "8. http://cs231n.github.io/convolutional-networks/\n",
    "9. http://slazebni.cs.illinois.edu/spring17/lec20_rnn.pdf\n",
    "10. https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/\n",
    "11. https://www.fast.ai/2018/07/23/auto-ml-3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
